"""
Auditor√≠a de Ausentismos - Parte 4
An√°lisis de Registros √önicos y Ventana de 30 D√≠as con Ponderaci√≥n

Filtro de Registros √önicos por C√≥digos de Ausentismo
Extrae registros √∫nicos por id_personal filtrados por c√≥digos espec√≠ficos
Luego aplica an√°lisis de 30 d√≠as con ponderaci√≥n espec√≠fica (25% por columna)
"""

import pandas as pd
import numpy as np
import os

# ============================================================================
# CONFIGURACI√ìN GLOBAL
# ============================================================================

# Rutas (se configurar√°n desde app.py)
ruta_entrada = ""
directorio_salida = ""
ruta_salida_unicos = ""
ruta_salida_30dias = ""
# Filtro opcional por fecha_ultima (last_approval_status_date)
fecha_ultima_inicio = None
fecha_ultima_fin = None

# C√≥digos a excluir de registros √∫nicos (con el 215)
CODIGOS_EXCLUIR_UNICOS = [203, 202, 216, 210, 220, 201, 200, 383, 215]

# C√≥digos a incluir en reporte 30 d√≠as (CON el 215)
CODIGOS_INCLUIR_30DIAS = [203, 202, 215, 216, 210, 220, 201, 200, 383]

# PONDERACIONES: 25% cada columna
COLUMNAS_PONDERADAS = {
    'GRUPO': 0.25,
    'Clasificaci√≥n Sistemas JMC': 0.25,
    'SEGMENTO': 0.25,
    'Clasificaci√≥n Partes JMC': 0.25
}

# Ventana de d√≠as para an√°lisis
VENTANA_DIAS = 30

# Ruta al archivo de c√≥digos en el repositorio
RUTA_CODIGOS_CSV = "datos_numericos.csv"

# ============================================================================
# FUNCI√ìN PRINCIPAL
# ============================================================================

def procesar_analisis_completo():
    """
    Ejecuta el an√°lisis completo:
    1. Filtra registros √∫nicos por c√≥digos
    2. Analiza ventana de 30 d√≠as con ponderaci√≥n

    Returns:
        tuple: (df_unicos, df_reporte_30dias) o (None, None) si hay error
    """

    print("=" * 80)
    print("PROCESAMIENTO DE REGISTROS √öNICOS Y AN√ÅLISIS 30 D√çAS")
    print("=" * 80)

    # DEBUG: Verificar configuraci√≥n inicial
    print("\nüîç DEBUG - Configuraci√≥n inicial:")
    print(f"  - ruta_entrada: {ruta_entrada}")
    print(f"  - directorio_salida: {directorio_salida}")
    print(f"  - ruta_salida_unicos: {ruta_salida_unicos}")
    print(f"  - ruta_salida_30dias: {ruta_salida_30dias}")
    print(f"  - fecha_ultima_inicio: {fecha_ultima_inicio}")
    print(f"  - fecha_ultima_fin: {fecha_ultima_fin}")
    print(f"  - RUTA_CODIGOS_CSV: {RUTA_CODIGOS_CSV}")

    def normalizar_texto(valor):
        """Convierte valores mixtos a texto seguro para joins/comparaciones."""
        if pd.isna(valor):
            return ''
        valor_str = str(valor).strip()
        if valor_str.lower() in {'nan', 'none'}:
            return ''
        return valor_str

    def join_seguro(valores, separador):
        """Une valores heterog√©neos evitando TypeError por floats/NaN."""
        return separador.join(
            [normalizar_texto(v) for v in valores if normalizar_texto(v)]
        )

    try:
        # ============================================================================
        # PASO 1: FILTRAR Y OBTENER REGISTROS √öNICOS
        # ============================================================================
        print("\n1. Procesando registros √∫nicos...")

        # DEBUG: Verificar archivo de entrada
        if not ruta_entrada:
            raise ValueError("‚ùå ruta_entrada no est√° configurada")
        if not os.path.exists(ruta_entrada):
            raise FileNotFoundError(f"‚ùå No se encuentra el archivo: {ruta_entrada}")

        print(f"   üìÇ Leyendo archivo: {os.path.basename(ruta_entrada)}")
        # Leer c√≥digo diagn√≥stico como texto para evitar coerci√≥n a float/NaN
        df = pd.read_csv(
            ruta_entrada,
            encoding='utf-8-sig',
            dtype={'descripcion_general_external_code': 'string'}
        )
        print(f"   ‚úÖ Registros totales: {len(df):,}")
        print(f"   üìã Columnas encontradas: {len(df.columns)}")

        # DEBUG: Mostrar primeras columnas
        print(f"   üîç Primeras 5 columnas: {list(df.columns[:5])}")
        print(f"   üîç Todas las columnas ({len(df.columns)}): {list(df.columns)}")

        # COMPATIBILIDAD: Verificar si existe fse_fechas o Final Salario enfer.
        tiene_fse_fechas = 'fse_fechas' in df.columns
        tiene_final_salario = 'Final Salario enfer.' in df.columns
        print(f"   üîç Columna 'fse_fechas': {'S√ç' if tiene_fse_fechas else 'NO'}")
        print(f"   üîç Columna 'Final Salario enfer.': {'S√ç' if tiene_final_salario else 'NO'}")

        # FILTRAR PARA REGISTROS √öNICOS: EXCLUIR los c√≥digos especificados
        if 'homologacion_clase_de_ausentismo_ssf_vs_sap' not in df.columns:
            raise ValueError(f"‚ùå ERROR: Columna 'homologacion_clase_de_ausentismo_ssf_vs_sap' NO EXISTE en el archivo. Columnas disponibles: {list(df.columns)}")

        # Validar que existan las columnas CR√çTICAS requeridas
        columnas_criticas = ['id_personal', 'last_approval_status_date', 'start_date',
                             'descripcion_general_external_code']
        columnas_faltantes = [col for col in columnas_criticas if col not in df.columns]
        if columnas_faltantes:
            print(f"‚ùå ERROR CR√çTICO: Faltan columnas OBLIGATORIAS: {columnas_faltantes}")
            print(f"   Columnas disponibles: {list(df.columns)}")
            return None, None

        # Verificar/agregar columnas opcionales para mantener compatibilidad
        if 'external_name_label' not in df.columns:
            print("   ‚ö†Ô∏è ADVERTENCIA: Columna opcional 'external_name_label' NO existe (se crea con 'N/A')")
            df['external_name_label'] = 'N/A'
        else:
            print("   ‚úÖ Columna opcional 'external_name_label' encontrada")

        if 'cie10_descripcion' not in df.columns:
            print("   ‚ö†Ô∏è ADVERTENCIA: Columna opcional 'cie10_descripcion' NO existe (se crea vac√≠a)")
            df['cie10_descripcion'] = ''
        else:
            print("   ‚úÖ Columna opcional 'cie10_descripcion' encontrada")

        if 'end_date' not in df.columns:
            print("   ‚ö†Ô∏è ADVERTENCIA: Columna opcional 'end_date' NO existe (se crea vac√≠a)")
            df['end_date'] = ''
        else:
            print("   ‚úÖ Columna opcional 'end_date' encontrada")

        # Normalizar columnas de texto usadas en joins/formateo
        df['descripcion_general_external_code'] = df['descripcion_general_external_code'].map(normalizar_texto)
        df['external_name_label'] = df['external_name_label'].map(normalizar_texto)
        df['cie10_descripcion'] = df['cie10_descripcion'].map(normalizar_texto)

        # Convertir fechas una sola vez (acepta DD/MM/YYYY o YYYY-MM-DD)
        df['last_approval_status_date'] = pd.to_datetime(
            df['last_approval_status_date'],
            dayfirst=True,
            errors='coerce'
        )
        df['start_date'] = pd.to_datetime(
            df['start_date'],
            dayfirst=True,
            errors='coerce'
        )
        df['end_date'] = pd.to_datetime(
            df['end_date'],
            dayfirst=True,
            errors='coerce'
        )

        # Filtro opcional por fecha_ultima
        if fecha_ultima_inicio is not None and fecha_ultima_fin is not None:
            fu_inicio_dt = pd.to_datetime(fecha_ultima_inicio, errors='coerce')
            fu_fin_dt = pd.to_datetime(fecha_ultima_fin, errors='coerce')

            if pd.isna(fu_inicio_dt) or pd.isna(fu_fin_dt):
                print("   ‚ö†Ô∏è Filtro fecha_ultima ignorado por fechas inv√°lidas")
            else:
                registros_antes_filtro_fecha = len(df)
                df = df[
                    (df['last_approval_status_date'] >= fu_inicio_dt) &
                    (df['last_approval_status_date'] <= fu_fin_dt)
                ].copy()
                print(
                    f"   ‚úÖ Filtro fecha_ultima aplicado: {fu_inicio_dt.strftime('%d/%m/%Y')} ‚Üí "
                    f"{fu_fin_dt.strftime('%d/%m/%Y')} | {registros_antes_filtro_fecha:,} ‚Üí {len(df):,}"
                )
        elif (fecha_ultima_inicio is not None) != (fecha_ultima_fin is not None):
            print("   ‚ö†Ô∏è Filtro fecha_ultima incompleto (falta inicio o fin), se omite")

        # Filtrar para registros √∫nicos (ya con fechas convertidas)
        df_filtrado_unicos = df[~df['homologacion_clase_de_ausentismo_ssf_vs_sap'].isin(CODIGOS_EXCLUIR_UNICOS)].copy()
        print(f"   Registros excluyendo c√≥digos {CODIGOS_EXCLUIR_UNICOS}: {len(df_filtrado_unicos):,}")

        # Ordenar por: id_personal, last_approval_status_date (desc), start_date (desc)
        # As√≠ el registro con la fecha m√°s reciente en start_date quedar√° primero
        df_filtrado_unicos = df_filtrado_unicos.sort_values(
            by=['id_personal', 'last_approval_status_date', 'start_date'],
            ascending=[True, False, False]
        )

        # Tomar el primer registro de cada id_personal (que ahora es el m√°s reciente)
        df_unicos = df_filtrado_unicos.drop_duplicates(subset=['id_personal'], keep='first')

        print(f"   Registros √∫nicos (SIN c√≥digos filtrados): {len(df_unicos):,}")
        print(f"   ‚Üí Criterio: √öltima last_approval_status_date y start_date m√°s reciente")

        df_unicos.to_csv(ruta_salida_unicos, index=False, encoding='utf-8-sig', date_format='%d/%m/%Y')
        print(f"‚úÖ Guardado: {os.path.basename(ruta_salida_unicos)}")

        # FILTRAR PARA REPORTE 30 D√çAS: INCLUIR SOLO los c√≥digos especificados
        df_filtrado_30dias = df[df['homologacion_clase_de_ausentismo_ssf_vs_sap'].isin(CODIGOS_INCLUIR_30DIAS)].copy()
        print(f"   Registros CON c√≥digos {CODIGOS_INCLUIR_30DIAS}: {len(df_filtrado_30dias):,}")

        df_filtrado_30dias = df_filtrado_30dias.sort_values(
            by=['id_personal', 'start_date', 'last_approval_status_date'],
            ascending=[True, False, False]
        )
        df_filtrado_30dias_unicos = df_filtrado_30dias.drop_duplicates(subset=['id_personal'], keep='first')
        print(f"   IDs √∫nicos para reporte 30 d√≠as: {len(df_filtrado_30dias_unicos):,}")
        
        # ============================================================================
        # PASO 2: CARGAR MATRIZ DE C√ìDIGOS
        # ============================================================================
        print("\n2. Cargando matriz de c√≥digos CIE-10...")
        
        # Verificar si existe el archivo en el repositorio
        if not os.path.exists(RUTA_CODIGOS_CSV):
            print(f"‚ùå ERROR: No se encontr√≥ el archivo {RUTA_CODIGOS_CSV}")
            return None, None
        
        df_codigos = pd.read_csv(
            RUTA_CODIGOS_CSV,
            encoding='utf-8-sig',
            dtype={'C√≥digo': 'string'}
        )
        
        # Eliminar columna porcentaje_relacion si existe
        if 'porcentaje_relacion' in df_codigos.columns:
            df_codigos = df_codigos.drop('porcentaje_relacion', axis=1)

        df_codigos['C√≥digo'] = df_codigos['C√≥digo'].map(normalizar_texto)
        
        print(f"‚úÖ Columnas disponibles en matriz: {list(df_codigos.columns)}")
        
        # Verificar que las columnas ponderadas existen
        columnas_faltantes = [col for col in COLUMNAS_PONDERADAS.keys() if col not in df_codigos.columns]
        if columnas_faltantes:
            print(f"‚ùå ERROR: Faltan columnas en la matriz: {columnas_faltantes}")
            return None, None
        
        # ============================================================================
        # PASO 3: PREPARAR DATOS PARA AN√ÅLISIS 30 D√çAS
        # ============================================================================
        print("\n3. Preparando datos para an√°lisis 30 d√≠as...")
        print("   ‚ÑπÔ∏è Se usan los datos ya cargados y preprocesados una sola vez")

        # Obtener solo los IDs √∫nicos del filtro CON C√ìDIGOS (para reporte 30 d√≠as)
        ids_filtrados = df_filtrado_30dias_unicos['id_personal'].unique()

        print(f"‚úÖ IDs a procesar: {len(ids_filtrados):,}")
        print(f"‚úÖ Ponderaci√≥n configurada:")
        for col, peso in COLUMNAS_PONDERADAS.items():
            print(f"   ‚Ä¢ {col}: {peso*100:.0f}%")

        # Filtrar por IDs
        df_ausentismos = df[df['id_personal'].isin(ids_filtrados)].copy()

        # Validar que haya datos
        if len(df_ausentismos) == 0:
            print("‚ùå ERROR: No hay datos despu√©s de filtrar por IDs")
            return None, None

        print(f"‚úÖ Registros v√°lidos para an√°lisis: {len(df_ausentismos):,}")
        
        # ============================================================================
        # PASO 4: CREAR DICCIONARIO DE C√ìDIGOS
        # ============================================================================
        print("\n4. Creando diccionario de c√≥digos...")
        
        codigo_a_valores = {}
        for idx, row in df_codigos.iterrows():
            codigo = normalizar_texto(row['C√≥digo'])
            if not codigo:
                continue
            valores = {col: row[col] for col in COLUMNAS_PONDERADAS.keys()}
            codigo_a_valores[codigo] = valores
        
        print(f"‚úÖ {len(codigo_a_valores)} c√≥digos en diccionario")
        
        # ============================================================================
        # PASO 5: PROCESAR CADA ID_PERSONAL
        # ============================================================================
        print("\n5. Procesando an√°lisis de 30 d√≠as...")
        
        resultados = []
        id_actual = None
        
        for contador, id_pers in enumerate(ids_filtrados, 1):
            id_actual = id_pers
            # Obtener datos de este ID
            datos_id = df_ausentismos[df_ausentismos['id_personal'] == id_pers].copy()

            # PROTECCI√ìN: Verificar que haya datos para este ID
            if len(datos_id) == 0:
                print(f"  ‚ö†Ô∏è SALTANDO ID {id_pers} (#{contador}/{len(ids_filtrados)}): Sin datos")
                continue

            # PRIORIDAD 1: Buscar el start_date m√°s reciente
            # PRIORIDAD 2: Si hay empate, desempatar por last_approval_status_date m√°s reciente
            datos_id_ordenado = datos_id.sort_values(
                by=['start_date', 'last_approval_status_date'],
                ascending=[False, False]  # Ambos descendentes (m√°s reciente primero)
            )

            # PROTECCI√ìN: Verificar que datos_id_ordenado no est√© vac√≠o
            if len(datos_id_ordenado) == 0:
                print(f"  ‚ö†Ô∏è SALTANDO ID {id_pers} (#{contador}/{len(ids_filtrados)}): DataFrame vac√≠o tras ordenar")
                continue

            # Tomar el primer registro (start_date m√°s reciente)
            registro_ultimo = datos_id_ordenado.iloc[0]

            fecha_aprobacion_maxima = registro_ultimo['last_approval_status_date']
            codigo_ultima_fecha = normalizar_texto(registro_ultimo['descripcion_general_external_code'])
            start_date_ultimo = registro_ultimo['start_date']
            end_date_ultimo = registro_ultimo['end_date']
            # CORRECCI√ìN: pandas Series no tiene m√©todo .get(), usar in index
            if 'external_name_label' in registro_ultimo.index:
                external_label_ultimo = registro_ultimo['external_name_label']
            else:
                external_label_ultimo = 'N/A'
            
            # Calcular fecha l√≠mite (30 d√≠as antes del start_date)
            fecha_limite = start_date_ultimo - pd.Timedelta(days=VENTANA_DIAS)
            
            # Filtrar registros dentro de la ventana de 30 d√≠as
            datos_filtrados = datos_id[
                (datos_id['start_date'] >= fecha_limite) & 
                (datos_id['start_date'] <= start_date_ultimo)
            ].copy()
            
            # Calcular d√≠as transcurridos
            datos_filtrados['dias_transcurridos'] = (start_date_ultimo - datos_filtrados['start_date']).dt.days
            
            # Excluir el c√≥digo que choca (el del registro m√°s reciente)
            datos_filtrados_sin_choque = datos_filtrados[
                (datos_filtrados['descripcion_general_external_code'] != codigo_ultima_fecha) |
                (datos_filtrados['start_date'] != start_date_ultimo)
            ].copy()

            # ORDENAR por start_date de menor a mayor (m√°s antigua primero)
            datos_filtrados_sin_choque = datos_filtrados_sin_choque.sort_values('start_date', ascending=True)

            # Calcular duraci√≥n en d√≠as del c√≥digo que choca
            if pd.notna(end_date_ultimo) and pd.notna(start_date_ultimo):
                duracion_dias = (end_date_ultimo - start_date_ultimo).days + 1
            else:
                duracion_dias = 0

            # Crear tipo_concepto (el c√≥digo que choca con todos)
            tipo_concepto = f"{codigo_ultima_fecha}(start:{start_date_ultimo.strftime('%d/%m/%Y')},dias:{duracion_dias})({external_label_ultimo})"

            # Si no hay datos para comparar
            if len(datos_filtrados_sin_choque) == 0:
                resultados.append({
                    'id_personal': id_pers,
                    'fecha_ultima': fecha_aprobacion_maxima,  # Mantener como datetime
                    'start_date': start_date_ultimo,  # Mantener como datetime
                    'end_date': end_date_ultimo if pd.notna(end_date_ultimo) else pd.NaT,  # Mantener como datetime
                    'codigo_ultima_fecha': codigo_ultima_fecha,
                    'tipo_concepto': tipo_concepto,
                    'todos_codigos': '',
                    'detalle_codigos_con_fechas': '',
                    'cantidad_codigos': 0,
                    'comparaciones_detalle': '',
                    'porcentaje_relacion': 0.0,
                    'cie10_descripcion': ''
                })
                continue
            
            # Verificar si el c√≥digo que choca existe en la tabla
            if codigo_ultima_fecha not in codigo_a_valores:
                detalle_codigos = []
                cie10_descripciones = []
                
                for idx, row in datos_filtrados_sin_choque.iterrows():
                    cod = normalizar_texto(row['descripcion_general_external_code'])
                    sd = row['start_date'].strftime('%d/%m/%Y')
                    dias = row['dias_transcurridos']
                    # CORRECCI√ìN: pandas Series no tiene m√©todo .get()
                    external_label = row['external_name_label'] if 'external_name_label' in row.index else 'N/A'
                    cie10_desc = row['cie10_descripcion'] if 'cie10_descripcion' in row.index else ''
                    
                    detalle_codigos.append(f"{cod}(start:{sd},dias:{dias})({external_label})")
                    
                    if pd.notna(cie10_desc) and cie10_desc != '':
                        cie10_descripciones.append(f"({str(cie10_desc)})")
                
                todos_codigos = [
                    normalizar_texto(cod)
                    for cod in datos_filtrados_sin_choque['descripcion_general_external_code'].unique().tolist()
                    if normalizar_texto(cod)
                ]

                resultados.append({
                    'id_personal': id_pers,
                    'fecha_ultima': fecha_aprobacion_maxima,  # Mantener como datetime
                    'start_date': start_date_ultimo,  # Mantener como datetime
                    'end_date': end_date_ultimo if pd.notna(end_date_ultimo) else pd.NaT,  # Mantener como datetime
                    'codigo_ultima_fecha': codigo_ultima_fecha,
                    'tipo_concepto': tipo_concepto,
                    'todos_codigos': join_seguro(todos_codigos, ', '),
                    'detalle_codigos_con_fechas': ' | '.join(detalle_codigos),
                    'cantidad_codigos': len(todos_codigos),
                    'comparaciones_detalle': 'C√≥digo que choca no encontrado en tabla',
                    'porcentaje_relacion': 0.0,
                    'cie10_descripcion': join_seguro(cie10_descripciones, '|')
                })
                continue
            
            # Procesar comparaciones con PONDERACI√ìN
            detalle_codigos = []
            comparaciones_detalle = []
            porcentajes = []
            cie10_descripciones = []
            valores_ultima = codigo_a_valores[codigo_ultima_fecha]
            
            for idx, row in datos_filtrados_sin_choque.iterrows():
                cod = normalizar_texto(row['descripcion_general_external_code'])
                sd = row['start_date'].strftime('%d/%m/%Y')
                dias = row['dias_transcurridos']
                # CORRECCI√ìN: pandas Series no tiene m√©todo .get()
                external_label = row['external_name_label'] if 'external_name_label' in row.index else 'N/A'
                cie10_desc = row['cie10_descripcion'] if 'cie10_descripcion' in row.index else ''

                # Verificar si el c√≥digo tiene caracteres especiales
                # CORRECCI√ìN: Verificar que cod no sea None y manejar casos especiales
                cod_str = str(cod).strip() if cod is not None else ''
                if not cod_str or '*' in cod_str or not cod_str.replace(' ', '').replace('.', '').isalnum():
                    detalle_codigos.append(f"{cod}(start:{sd},dias:{dias},error_codigo)({external_label})")
                    comparaciones_detalle.append(f"{cod}:error_codigo")
                else:
                    detalle_codigos.append(f"{cod}(start:{sd},dias:{dias})({external_label})")
                    
                    # Verificar si el c√≥digo existe en el diccionario
                    if cod in codigo_a_valores:
                        valores_hist = codigo_a_valores[cod]
                        
                        # CALCULAR PORCENTAJE PONDERADO: 25% por cada columna
                        porcentaje_total = 0.0
                        
                        for columna, peso in COLUMNAS_PONDERADAS.items():
                            # Si coincide la columna, sumar el 25%
                            if valores_ultima[columna] == valores_hist[columna]:
                                porcentaje_total += (peso * 100)
                        
                        porcentajes.append(porcentaje_total)
                        comparaciones_detalle.append(f"{cod}:{porcentaje_total:.1f}%")
                    else:
                        comparaciones_detalle.append(f"{cod}:N/A")
                
                # Agregar descripci√≥n CIE-10 si existe con formato |(DESCRIPCION)|
                if pd.notna(cie10_desc) and cie10_desc != '':
                    cie10_descripciones.append(f"({str(cie10_desc)})")
            
            # Crear strings de detalle
            detalle_str = ' | '.join(detalle_codigos)
            comparaciones_str = ' | '.join(comparaciones_detalle)
            todos_codigos = [
                normalizar_texto(cod)
                for cod in datos_filtrados_sin_choque['descripcion_general_external_code'].unique().tolist()
                if normalizar_texto(cod)
            ]
            
            # Calcular promedio de porcentajes
            porcentaje_promedio = np.mean(porcentajes) if porcentajes else 0.0

            # Guardar resultado
            resultados.append({
                'id_personal': id_pers,
                'fecha_ultima': fecha_aprobacion_maxima,  # Mantener como datetime
                'start_date': start_date_ultimo,  # Mantener como datetime
                'end_date': end_date_ultimo if pd.notna(end_date_ultimo) else pd.NaT,  # Mantener como datetime
                'codigo_ultima_fecha': codigo_ultima_fecha,
                'tipo_concepto': tipo_concepto,
                'todos_codigos': join_seguro(todos_codigos, ', '),
                'detalle_codigos_con_fechas': join_seguro([detalle_str], ' | '),
                'cantidad_codigos': len([c for c in todos_codigos if normalizar_texto(c)]),
                'comparaciones_detalle': join_seguro([comparaciones_str], ' | '),
                'porcentaje_relacion': round(porcentaje_promedio, 2),
                'cie10_descripcion': join_seguro(cie10_descripciones, '|')
            })
            
            # Mostrar progreso
            if contador % 500 == 0:
                print(f"  Procesados {contador}/{len(ids_filtrados)} IDs...")
        
        print(f"‚úÖ Procesamiento completado")
        
        # ============================================================================
        # PASO 6: GUARDAR REPORTE 30 D√çAS
        # ============================================================================
        print("\n6. Guardando reporte 30 d√≠as...")
        
        df_resultado = pd.DataFrame(resultados)
        
        # NOMBRES DE COLUMNAS CORRECTOS Y EN ESPA√ëOL
        columnas_orden = [
            'id_personal',
            'fecha_ultima',
            'start_date',
            'end_date',
            'codigo_ultima_fecha',
            'tipo_concepto',
            'todos_codigos',
            'detalle_codigos_con_fechas',
            'cantidad_codigos',
            'comparaciones_detalle',
            'porcentaje_relacion',
            'cie10_descripcion'
        ]
        
        df_resultado = df_resultado[columnas_orden]
        
        # Guardar CSV con formato CORRECTO y fechas en DD/MM/YYYY
        df_resultado.to_csv(
            ruta_salida_30dias,
            index=False,
            sep=';',
            encoding='utf-8-sig',
            decimal=',',
            date_format='%d/%m/%Y',  # Formato d√≠a/mes/a√±o para fechas
            quoting=1,
            lineterminator='\n'
        )
        
        print(f"‚úÖ Guardado: {os.path.basename(ruta_salida_30dias)}")
        
        # ============================================================================
        # PASO 7: ESTAD√çSTICAS FINALES
        # ============================================================================
        print("\n" + "=" * 80)
        print("RESUMEN FINAL")
        print("=" * 80)
        
        print(f"\nüìä Archivos generados:")
        print(f"  1. {os.path.basename(ruta_salida_unicos)}: {len(df_unicos):,} registros")
        print(f"     ‚Üí Registros √∫nicos EXCLUYENDO c√≥digos {CODIGOS_EXCLUIR_UNICOS}")
        print(f"  2. {os.path.basename(ruta_salida_30dias)}: {len(df_resultado):,} registros")
        print(f"     ‚Üí An√°lisis 30 d√≠as SOLO con c√≥digos {CODIGOS_INCLUIR_30DIAS}")
        
        print(f"\nüìà Estad√≠sticas reporte 30 d√≠as:")
        print(f"  IDs con c√≥digos para comparar: {len(df_resultado[df_resultado['cantidad_codigos'] > 0]):,}")
        print(f"  IDs sin c√≥digos para comparar: {len(df_resultado[df_resultado['cantidad_codigos'] == 0]):,}")
        print(f"  Porcentaje promedio: {df_resultado['porcentaje_relacion'].mean():.2f}%")
        
        print(f"\nüí° Ponderaci√≥n aplicada:")
        for col, peso in COLUMNAS_PONDERADAS.items():
            print(f"  ‚Ä¢ {col}: {peso*100:.0f}%")
        print(f"  ‚Üí Total posible: 100% (si coinciden las 4 columnas)")
        
        print("\n‚úÖ PROCESO COMPLETADO")
        print("=" * 80)
        
        return df_unicos, df_resultado
    
    except Exception as e:
        print("\n" + "=" * 80)
        print("‚ùå ERROR CR√çTICO EN PROCESAMIENTO")
        print("=" * 80)
        print(f"\nüî¥ Tipo de Error: {type(e).__name__}")
        print(f"üî¥ Mensaje: {str(e)}")
        if 'id_actual' in locals() and id_actual is not None:
            print(f"üî¥ √öltimo id_personal procesado: {id_actual}")
        print("\nüìç TRACEBACK COMPLETO:")
        print("-" * 80)
        import traceback
        print(traceback.format_exc())
        print("-" * 80)
        print("\nüí° INFORMACI√ìN DE DEBUG:")
        print(f"  - Archivo de entrada existe: {os.path.exists(ruta_entrada) if ruta_entrada else 'NO CONFIGURADO'}")
        print(f"  - Archivo c√≥digos existe: {os.path.exists(RUTA_CODIGOS_CSV)}")
        print(f"  - Directorio salida: {directorio_salida if directorio_salida else 'NO CONFIGURADO'}")
        print("=" * 80)
        return None, None


# ============================================================================
# EJECUCI√ìN DIRECTA (PARA PRUEBAS LOCALES)
# ============================================================================

if __name__ == "__main__":
    # Configuraci√≥n de ejemplo para ejecuci√≥n local
    ruta_entrada = r"C:\Users\jjbustos\Downloads\PASO_3_CIE10\ausentismos_completo_con_cie10.csv"
    directorio_salida = r"C:\Users\jjbustos\Downloads\salida"
    ruta_salida_unicos = os.path.join(directorio_salida, "Registros_unicos.csv")
    ruta_salida_30dias = os.path.join(directorio_salida, "reporte_30_dias.csv")
    
    # Crear directorio de salida si no existe
    os.makedirs(directorio_salida, exist_ok=True)
    
    # Ejecutar proceso
    df_unicos, df_reporte = procesar_analisis_completo()
    
    if df_unicos is not None and df_reporte is not None:
        print("\n‚úÖ Archivos generados correctamente")
    else:
        print("\n‚ùå Error en el procesamiento")
